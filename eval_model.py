import argparse
import random
import time
import numpy as np
import torch
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM
from model import SpongeBob
from Config import LLMConfig


def init_model(args):
     tokenizer = AutoTokenizer.from_pretrained('./spongebob_tokenizer')
     if args.model_mode == 2:
          ckp = f'./{args.save_dir}/distill_long.pth'
     if args.model_mode == 1:
          ckp = f'./{args.save_dir}/SFT.pth'
     if args.model_mode == 0:
          ckp = f'./{args.save_dir}/pretrain.pth'

     model = SpongeBob(LLMConfig(
          max_seq_len=args.max_seq_len,
     ))

     state_dict = torch.load(ckp, map_location=args.device)
     model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=True)

     print(f'æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M(illion)')
     return model.eval().to(args.device), tokenizer




def main():
     parser=argparse.ArgumentParser()
     parser.add_argument('--save_dir', default='results', type=str)
     parser.add_argument('--temperature', default=0.85, type=float)
     parser.add_argument('--top_p', default=0.85, type=float)
     parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
     parser.add_argument('--max_seq_len', default=8192, type=int)
     parser.add_argument('--history_cnt', default=0, type=int)
     parser.add_argument('--stream', default=True, type=bool)
     parser.add_argument('--model_mode', default=1, type=int,
                        help="0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹")
     
     args = parser.parse_args()
     model, tokenizer = init_model(args)
     messages=[]

     while True:
          # è·å–ç”¨æˆ·è¾“å…¥
          prompt = input('ğŸ‘¶: ')  # æ‰‹åŠ¨è¾“å…¥å¯¹è¯å†…å®¹

          messages = messages[-args.history_cnt:] if args.history_cnt else []
          messages.append({"role": "user", "content": prompt})

          new_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
          )[-args.max_seq_len + 1:] if args.model_mode != 0 else (tokenizer.bos_token + prompt)
          print('new_prompt:', new_prompt)
          
          with torch.no_grad():
               x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=args.device).unsqueeze(0)
               outputs = model.generate(
                    x,
                    eos_token_id=tokenizer.eos_token_id,
                    max_new_tokens=args.max_seq_len,
                    temperature=args.temperature,
                    top_p=args.top_p,
                    stream=True,
                    pad_token_id=tokenizer.pad_token_id,
                    rp=1.3
               )

               print('ğŸ¤–ï¸: ', end='')  # æ‰“å°æœºå™¨äººå›¾æ ‡ï¼Œä¸æ¢è¡Œ
               try:
                    history_idx = 0  # åˆå§‹åŒ–å†å²ç´¢å¼•ï¼Œç”¨äºè·Ÿè¸ªå·²æ‰“å°çš„ç­”æ¡ˆé•¿åº¦
                    for y in outputs:  # éå†æ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ªè¾“å‡º
                         # y[0]ä¸ºinput_ids
                         answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)  # è§£ç è¾“å‡ºï¼Œè·³è¿‡ç‰¹æ®Šæ ‡è®°
                         if (answer and answer[-1] == 'ï¿½') or not answer:  # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æœ‰æ•ˆ
                              continue  # å¦‚æœç­”æ¡ˆæ— æ•ˆï¼Œåˆ™è·³è¿‡å½“å‰å¾ªç¯
                         print(answer[history_idx:], end='', flush=True)  # æ‰“å°æœ‰æ•ˆç­”æ¡ˆçš„å‰©ä½™éƒ¨åˆ†ï¼Œä¸æ¢è¡Œ
                         history_idx = len(answer)  # æ›´æ–°å†å²ç´¢å¼•ä¸ºå½“å‰ç­”æ¡ˆçš„é•¿åº¦
               except StopIteration:  # æ•è·åœæ­¢è¿­ä»£å¼‚å¸¸
                    print("No answer")  # å¦‚æœæ²¡æœ‰ç­”æ¡ˆï¼Œæ‰“å°æç¤ºä¿¡æ¯
               print('\n')

          messages.append({"role": "assistant", "content": answer})


if __name__ == "__main__":
    main()